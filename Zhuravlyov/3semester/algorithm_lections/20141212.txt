Продолжаем про кеш процессора.

В прошлый раз мы рассматривали пример с суой элементов массива: подряд считается намного быстрее, чем в случ. порядке.

Если вы рассматрваете матрицу, то
for i = 1 to n do
for j = 1 to n do
a[i][j] - работает намного быстрее, чем a[j][i]
Это потому что в первом случае вы выгружаете первый массив и рассматриваете его, а во втором так сделать не получится.

Ещё один классический пример - пример с перемножением матриц.
c[i][j]=sum(a[i][t]*b[t][j])
Здесь обращение к матрице B невыгодно. Лучше матрицу B транспонировать, тогда перемножение будет намного быстрее.

Даже если асимптотически 2 алгоритма работают за одинаковое время. Если один из них всё время работает в 20 раз быстрее, то это очень существенно.

Рассмотрим бинарный поиск.

Он работает за O(logn). Асимптотически быстрее его не реализовать.
Но кешу этот алгоритм не нравится, т.к. алгоритм всё время перемещается между разными элементами массива.
То есть количество кеш-миссов - это O(logn)

Пусть у вас есть задача: есть отсортированный массив, в котором мы хотим найти каждый элемент.
Асимптотика работы: O(nlogn), т.к. n раз мы делаем бин. поиск.
На самом деле рост времени работы будет не ровно пропорционален nlogn. В какой-то момент у вас произойдёт скачок.
При маленьких n весь массив помещается в кеш процессора и количество кеш-миссов мало. При больших n почти все запросы приводят к кеш-миссу.

Что же с этим делать?
В самом деле, можно разбить массив на некоторое количество блоков так, чтобы каждый блок был достаточно маленьким, чтобы помещаться в кеш процессора.
Для каждого блока мы будем храниь его минимальный элемент. Выпишем все эти минимальные элементы в отдельный массив.
Тогда бинарным поиском найдём в последнем массиве номер блока и в рамках уже этого блока проведём бинарный поиск.
Грубо говоря, у нас есть массив размера 10^6. Мы разбиваем его на 10^3 массивов по 10^3.
В первом случаяе у нас log(10^6)=6log(10) операций.
Во втором случае log(10^3)+log(10^3) = 6log(10) операций. 
Казалось бы алгоритмы работают одинаково. Однако второй вариант существенно быстрее, т.к. куски массива мелкого размера будут помещаться в кеш процессора.
В первом случае кеш-миссов примерно столько же, сколько всего операций.
Во втором случае, хотя бы при прогоне по первому массиву кеш-миссов точно не будет.
Вообще параметры разбиения нужно подгонять в зависимости от конкретного размера кеша.
Если вышло так, что размер массива ещё больше, чем 10^6, то, возожно, рациональным будет по аналогии надстроить третий слой.

Всё вышесказанное распространяется и на жёсткий диск тоже: последовательный доступ существенно быстрее случайного.

Поговорим о деревьях
Вообще считается, что двоичные деревья выгоднее остальных, т.к. у k-ичного дерев операция работает за O(klog|(k)n)
Пусть дерево хранится в памяти. Тогда каждая операция перехода требует случайного обращения в память.
С этой точки зрения b-деревья лучше: k-ичные деревья, где у предка хранится ссылка на массив сыновей.
Тогда переход на нижний уровень в целом будет работать за столько же (обратиться в памяти к одному элементу или к k по сути роли не играет)
Однако сокращение уже в k-ичный логарифм, так что в целом выходит эффективнее.
Так что деревья во внешней памяти обычно хранятся как b-деревья.

Ну, в целом описанная конструкция применима и к оперативной памяти, т.к. с кешом процессора дела обстоят примерно так же.
Поэтому с точки зрения кеша k-ичная куча выгодней бинарной кучи.

В принципе, на практике может случиться так, что алгоритм для O(n^2) будет работать быстрее, чем алгоритм за O(nlogn) до какого-то n.

Сейчас процессоры очень быстрые, а память за процессором не успевает. Поэтому сейчас можно сказать эра, когда время работы алгоритма - это количество его кеш-миссов.